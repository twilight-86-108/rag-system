from numpy import size
import streamlit as st
from pathlib import Path
import tempfile
from typing import List, Optional
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¿½åŠ 
sys.path.append(str(Path(__file__).parent))
# ä½œæˆã—ãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from rag.document_loader import DocumentLoderFactory
from rag.text_splitter import DocumentChunker, TextSplitterConfig
from rag.vector_store import VectorStoreManager, VectorStoreConfig
from rag.llm_integration import LLMManager
from rag.retrieval import RAGPipeline, RetrieverConfig

st.set_page_config(
    page_title="RAG system",
    page_icon="ğŸ‡·ğŸ‡¸",
    layout="wide",
    initial_sidebar_state="expanded",
)


def init_session_state():
    """
    å…¨ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹å¤‰æ•°ã‚’åˆæœŸåŒ–
    """
    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = None
    if "rag_pipeline" not in st.session_state:
        st.session_state.rag_pipeline = None
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = []
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []


def render_header():
    """
    ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼
    """
    st.title("RAG system")
    st.markdown(
        """
    **Document Question & Answer System**
    
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­å®šã—ã¦ç‹¬è‡ªã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«åŸºã¥ã„ãŸ
    AIå›ç­”ã‚’ç”Ÿæˆã€‚
    """
    )
    col1, col2, col3 = st.columns(3)

    with col1:
        if st.session_state.uploaded_files:
            st.metric(
                "èª­ã¿è¾¼ã¿æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ",
                len(st.session_state.uploaded_files),
                delta="æº–å‚™å®Œäº†",
            )
        else:
            st.metric("èª­ã¿è¾¼ã¿æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ", "0", delta="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¿…è¦")

    with col2:
        if st.session_state.vectorstore:
            st.metric("vectorstore", "ç¨¼åƒä¸­", delta="æº–å‚™å®Œäº†")
        else:
            st.metric("vectorstore", "æœªä½œæˆ", delta="å‡¦ç†ãŒå¿…è¦")

    with col3:
        if st.session_state.rag_pipeline:
            st.metric("RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³", "æº–å‚™å®Œäº†", delta="ç¨¼åƒä¸­")
        else:
            st.metric("RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³", "æœªæº–å‚™", delta="å‡¦ç†ãŒå¿…è¦")

        st.divider()


def render_sidebar():
    """
    è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãã‚µã‚¤ãƒ‰ãƒãƒ¼
    """
    st.sidebar.title("è¨­å®š")

    st.sidebar.header("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.sidebar.file_uploader(
        "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰(PDFã¾ãŸã¯TXT)",
        type=["pdf", "txt"],
        accept_multiple_files=True,
        help="çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã«1ã¤ä»¥ä¸Šã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
    )

    st.sidebar.header("å‡¦ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")

    chunk_size = st.sidebar.slider(
        "ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º",
        min_value=500,
        max_value=2000,
        value=1000,
        step=100,
        help="å‡¦ç†æ™‚ã®ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º",
    )

    chunk_overlap = st.sidebar.slider(
        "ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—",
        min_value=0,
        max_value=500,
        value=200,
        step=50,
        help="é€£ç¶šã™ã‚‹ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—",
    )

    st.sidebar.header("æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")

    k = st.sidebar.slider(
        "æ¤œç´¢çµæœæ•°ï¼ˆkï¼‰",
        min_value=1,
        max_value=10,
        value=3,
        help="æ¤œç´¢ã™ã‚‹é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã®æ•°",
    )

    temperature = st.sidebar.slider(
        "Temperature",
        min_value=0.0,
        max_value=1.0,
        value=0.0,
        step=0.1,
        help="LLMã®å‰µé€ æ€§è¨­å®š(0=å›ºå®šã€1=å¤‰å‹•çš„)",
    )

    process_button = st.sidebar.button(
        "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç®¡ç†",
        disabled=(uploaded_files is None or len(uploaded_files) == 0),
        use_container_width=True,
        type="primary",
    )

    with st.sidebar.expander("ãƒ‡ãƒãƒƒã‚°æƒ…å ±"):
        st.json(
            {
                "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«": (
                    len(uploaded_files) if uploaded_files else 0
                ),
                "ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º": chunk_size,
                "ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—": chunk_overlap,
                "k": k,
                "temperature": temperature,
                "ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ç¨¼åƒ": st.session_state.vectorstore is not None,
                "RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¨¼åƒ": st.session_state.rag_pipeline is not None,
            }
        )

    return uploaded_files, chunk_size, chunk_overlap, k, temperature, process_button


def process_uploaded_files(
    uploaded_files: List, chunk_size: int, chunk_overlap: int
) -> Optional[VectorStoreManager]:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã€ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã™ã‚‹

    Arguments:
        uploaded_files: Streamlitã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€è¦§
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        chunk_overlap: ãƒãƒ£ãƒ³ã‚¯ã®é‡è¤‡

    Returns:
        VectorStoreManagerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹None(å‡¦ç†ãŒå¤±æ•—ã—ãŸå ´åˆ)
    """
    try:
        all_documents = []

        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)

            progress_bar = st.progress(0)
            status_text = st.empty()

            for i, uploaded_file in enumerate(uploaded_files):
                status_text.text(f"{uploaded_file.name}ã‚’èª­ã¿è¾¼ã¿ä¸­...")

                file_path = temp_path / uploaded_file.name
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())

                documents = DocumentLoderFactory.load_documents(str(file_path))
                all_documents.extend(documents)

                progress_bar.progress((i + 1) / (len(uploaded_files) * 3))

        if not all_documents:
            st.error("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return None

        st.success(f"{len(all_documents)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

        status_text.text("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ä¸­...")
        config = TextSplitterConfig(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        chunker = DocumentChunker(config=config)
        split_docs = chunker.chunk_documents(all_documents)
        progress_bar.progress(2 / 3)
        st.success(f"{len(split_docs)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

        status_text.text("Vector Storeã‚’ä½œæˆä¸­...")
        vectorstore_manager = VectorStoreManager()
        vectorstore = vectorstore_manager.create_vectorstore(split_docs)
        progress_bar.progress(1.0)
        st.success(f"{len(split_docs)}å€‹ã®åŸ‹ã‚è¾¼ã¿ã§Vector Storeã‚’ä½œæˆã—ã¾ã—ãŸã€‚")

        progress_bar.empty()
        status_text.empty()

        return vectorstore

    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        import traceback

        with st.expander("ã‚¨ãƒ©ãƒ¼è©³ç´°"):
            st.code(traceback.format_exc())
        return None


def create_retrieval_chain(
    vectorstore, k: int, temperature: float
) -> Optional[RAGPipeline]:
    """
    RAGæ¤œç´¢ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ

    Arguments:
        vectorstore: VectorStoreã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        k: å–å¾—ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
        temperature: LLMã®temperatureè¨­å®š

    Returns:
        RAGPipelineã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ or None(å¤±æ•—ã—ãŸå ´åˆ)
    """
    try:
        with st.spinner("RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–ä¸­..."):
            llm_manager = LLMManager(temperature=temperature)

            retriever_config = RetrieverConfig(k=k)
            rag_pipeline = RAGPipeline(
                vectorstore=vectorstore,
                llm_manager=llm_manager,
                retriever_config=retriever_config,
            )
            st.success("RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æº–å‚™å®Œäº†")
            return rag_pipeline

    except Exception as e:
        st.error(f"æ¤œç´¢ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        import traceback

        with st.expander("ã‚¨ãƒ©ãƒ¼è©³ç´°"):
            st.code(traceback.format_exc())
        return None


def render_qa_interface():
    """
    Q&A ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    """
    if not st.session_state.rag_pipeline:
        st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å‡¦ç†ã—ã¦ãã ã•ã„")
        st.markdown(
            """
        ### é–‹å§‹æ–¹æ³•:
        1. ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’ã‚¯ãƒªãƒƒã‚¯
        2. PDF or TXT ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        3. ã€Œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
        4. ã“ã“ã§è³ªå•
        """
        )
        return

    question = st.text_input(
        "è³ªå•ã‚’å…¥éŒ²ã—ã¦ãã ã•ã„:",
        placeholder="ä¾‹: ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸»ãªãƒˆãƒ”ãƒƒã‚¯ã¯ï¼Ÿ",
        help="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦è³ªå•",
        key="question_input",
    )

    col1, col2, col3 = st.columns([2, 2, 3])
    with col1:
        ask_button = st.button("è³ªå•ã™ã‚‹", use_container_width=True, type="primary")
    with col2:
        show_sources = st.checkbox("ã‚½ãƒ¼ã‚¹è¡¨ç¤º", value=True)
    with col3:
        clear_button = st.button("å±¥æ­´ã‚¯ãƒªã‚¢", use_container_width=True)

    if clear_button:
        st.session_state.chat_history = []
        st.rerun()

    if ask_button and question:
        with st.spinner("è€ƒãˆä¸­..."):
            try:
                """
                if show_sources:
                    result = st.session_state.rag_pipeline.query_with_sources(question)
                    answer = result["answer"]
                    sources = result["sources"]
                else:
                    answer = st.session_state.rag_pipeline.query_with_sources(question)
                    sources = []

                st.session_state.chat_history.append(
                    {"question": question, "answer": answer, "sources": sources}
                )
                """
                st.markdown("**è³ªå•**")
                st.write(question)

                st.markdown("**å›ç­”**")

                if show_sources:
                    result = st.session_state.rag_pipeline.query_stream_with_sources(
                        question
                    )
                    sources = result["sources"]
                    stream = result["stream"]

                    answer = st.write_stream(stream)
                else:
                    stream = st.session_state.rag_pipeline.query_stream(question)
                    answer = st.write_stream(stream)
                    sources = []

                st.session_state.chat_history.append(
                    {"question": question, "answer": answer, "sources": sources}
                )

            except Exception as e:
                st.error(f"è³ªå•å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                import traceback

                with st.expander("ã‚¨ãƒ©ãƒ¼è©³ç´°"):
                    st.code(traceback.format_exc())
    if st.session_state.chat_history:
        st.divider()
        st.subheader("ä¼šè©±å±¥æ­´")

        for i, exchange in enumerate(reversed(st.session_state.chat_history)):
            with st.expander(
                f"Q{len(st.session_state.chat_history) - i}: {exchange['question'][:80]}...",
                expanded=(i == 0),
            ):
                st.markdown("**è³ªå•**")
                st.write(exchange["question"])

                st.markdown("**å›ç­”**")
                st.write(exchange["answer"])

                if exchange.get("sources"):
                    st.markdown(
                        f"**å‚ç…§å…ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ({len(exchange['sources'])}ä»¶):**"
                    )
                    for j, doc in enumerate(exchange["sources"], 1):
                        with st.container():
                            st.markdown(f"**ã‚½ãƒ¼ã‚¹ {j}:**")
                            preview = doc.page_content[:300]
                            if len(doc.page_content) > 300:
                                preview += "..."
                            st.text(preview)

                            if doc.metadata:
                                st.caption(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {doc.metadata}")

                            st.markdown("---")


def render_sample_queries():
    """
    ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã‚»ã‚¯ã‚·ãƒ§ãƒ³
    """
    if not st.session_state.rag_pipeline:
        return

    st.header("ã‚µãƒ³ãƒ—ãƒ«è³ªå•")
    st.markdown("ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã§ãŠè©¦ã—:")

    sample_queries = [
        "ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¸»ãªãƒˆãƒ”ãƒƒã‚¯ã¯ï¼Ÿ",
        "ä¸»è¦äº‹é …ã‚’è¦ç´„ã—ã¦",
        "ã©ã®ã‚ˆã†ãªæŠ€è¡“çš„è©³ç´°ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ï¼Ÿ",
        "é‡è¦ãªãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚‹ï¼Ÿ",
    ]

    def set_query(q):
        st.session_state.question_input = q

    cols = st.columns(2)
    for i, query in enumerate(sample_queries):
        with cols[i % 2]:
            st.button(
                query,
                key=f"sample{i}",
                use_container_width=True,
                on_click=set_query,
                args=(query,),
            )


def render_footer():
    """
    ã‚¢ãƒ—ãƒªã®ãƒ•ãƒƒã‚¿ãƒ¼
    """
    st.divider()
    st.markdown(
        """
    <div style='text-align: center; color: #666; padding: 20px:'>
        <small>
        <strong>RAG system</strong>
        </small>
    </div>
    """,
        unsafe_allow_html=True,
    )


def main():
    """
    ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    """
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–
    init_session_state()
    # ãƒ˜ãƒƒãƒ€ãƒ¼èª­ã¿è¾¼ã¿
    render_header()
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼èª­ã¿è¾¼ã¿ã€è¨­å®šå–å¾—
    uploaded_files, chunk_size, chunk_overlap, k, temperature, process_button = (
        render_sidebar()
    )

    if process_button and uploaded_files:
        st.session_state.uploaded_files = [f.name for f in uploaded_files]

        vectorstore = process_uploaded_files(uploaded_files, chunk_size, chunk_overlap)

        if vectorstore:
            st.session_state.vectorstore = vectorstore

            rag_pipeline = create_retrieval_chain(vectorstore, k, temperature)

            if rag_pipeline:
                st.session_state.rag_pipeline = rag_pipeline
                st.balloons()
                st.rerun()

    # ã‚¿ãƒ–ã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    tab1, tab2, tab3 = st.tabs(["è³ªå•å¿œç­”", "ã‚µãƒ³ãƒ—ãƒ«", "ãƒ˜ãƒ«ãƒ—"])

    with tab1:
        render_qa_interface()

    with tab2:
        render_sample_queries()

    with tab3:
        st.markdown(
            """
        ## ä½¿ã„æ–¹

        ### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        1. ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’é–‹ãï¼ˆéš ã‚Œã¦ã„ã‚‹å ´åˆã¯â˜°ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼‰
        2. ã€Œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã§PDFã¾ãŸã¯TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
        3. ã€Œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        4. å‡¦ç†ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…ã¤

        ### ã‚¹ãƒ†ãƒƒãƒ—2: è³ªå•ã™ã‚‹
        1. ã€Œè³ªå•å¿œç­”ã€ã‚¿ãƒ–ã«ç§»å‹•
        2. ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«è³ªå•ã‚’å…¥åŠ›
        3. ã€Œè³ªå•ã™ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        4. AIç”Ÿæˆã®å›ç­”ã‚’ç¢ºèª
        
        ### ã‚¹ãƒ†ãƒƒãƒ—3: çµæœã‚’ç¢ºèª
        - è³ªå•ã®ä¸‹ã«å›ç­”ãŒè¡¨ç¤ºã•ã‚Œã¾ã™
        - ã€Œã‚½ãƒ¼ã‚¹è¡¨ç¤ºã€ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã¨ä½¿ç”¨ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã§ãã¾ã™
        - éå»ã®è³ªå•ã¯ä¼šè©±å±¥æ­´ã«è‡ªå‹•ä¿å­˜ã•ã‚Œã¾ã™
        - ã€Œå±¥æ­´ã‚¯ãƒªã‚¢ã€ã§ãƒªã‚»ãƒƒãƒˆå¯èƒ½

        ---

        ## è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³
        
        ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ä»¥ä¸‹ã®è¨­å®šã‚’èª¿æ•´ã§ãã¾ã™:
        
        **å‡¦ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**
        - **ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º**: ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®ã‚µã‚¤ã‚ºï¼ˆ500-2000æ–‡å­—ï¼‰
        - **ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—**: ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆ0-500æ–‡å­—ï¼‰
          - æ¨å¥¨: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®20%ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿æŒã‚’æ”¹å–„
        
        **æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**
        - **æ¤œç´¢çµæœæ•° (k)**: æ¤œç´¢ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°ï¼ˆ1-10ï¼‰
          - kãŒå¤§ãã„ã»ã©ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒå¤šã„ãŒå‡¦ç†ãŒé…ã„
        - **Temperature**: LLMã®å‰µé€ æ€§ãƒ¬ãƒ™ãƒ«ï¼ˆ0.0-1.0ï¼‰
          - 0.0 = å›ºå®šçš„ã€äº‹å®Ÿãƒ™ãƒ¼ã‚¹ã®å›ç­”
          - 1.0 = å¤‰å‹•çš„ã€å¤šæ§˜ãªå›ç­”
        
        ---
        """
        )

    # ãƒ•ãƒƒã‚¿ãƒ¼
    render_footer()


if __name__ == "__main__":
    main()
